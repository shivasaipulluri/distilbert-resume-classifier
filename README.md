# Resume Section Classifier (DistilBERT)

This project fine-tunes **DistilBERT** to classify segments of resumes into categories like  
*Education, Skills, Projects, Experience, Certifications, etc.*  

It complements the **Resume Segmenter (LLM Pipeline)** by training a lightweight transformer model for fast inference.

---

## ðŸ“Œ Project Overview
- **Input Dataset**: Created from structured JSON resumes.
  - `train.jsonl` (3024 samples)
  - `val.jsonl` (757 samples)
- **Model**: DistilBERT (`distilbert-base-uncased`) with classification head.
- **Training Setup**:
  - 5 epochs
  - Batch size = 8
  - Optimizer = AdamW
  - Metrics = Accuracy & Weighted F1

---

## ðŸ“Š Results
- **Validation Accuracy**: ~98.0%
- **Weighted F1 Score**: ~97.9%
- **Training Time**: ~6 hours on GPU

---

## ðŸ“‚ Repository Structure
distilbert-resume-classifier/
â”œâ”€â”€ prepare_dataset.ipynb # JSON â†’ train/val dataset
â”œâ”€â”€ train_resume_model.ipynb # Training notebook
â”œâ”€â”€ Distilbert_model.ipynb # Final model training & evaluation
â”œâ”€â”€ train.jsonl # Training dataset
â”œâ”€â”€ val.jsonl # Validation dataset
â”œâ”€â”€ resume-segmenter/ # Saved model & tokenizer
â””â”€â”€ README.md

---

## ðŸš€ Why This Project
- LLMs are powerful but costly for inference.
- DistilBERT offers **fast, efficient classification** suitable for deployment.
- Demonstrates ability to fine-tune state-of-the-art NLP models.

---

## ðŸ§  Skills Demonstrated
- Dataset engineering (`train.jsonl`, `val.jsonl`).
- Transformer fine-tuning with Hugging Face.
- Model evaluation (accuracy, F1).
- Saving & exporting models for deployment.
